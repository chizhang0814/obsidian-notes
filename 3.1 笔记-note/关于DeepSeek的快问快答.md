**问：DeepSeek 有什么核心创新？**
**答：
2024 年 12 月 DeepSeek-V3 发布：对标 GPT-4o 等大模型的基座模型
2025 年 01 月 DeepSeek-R1 发布：对标 OpenAI o1 的思维链推理模型**
DeepSeek 大模型依旧继承了 Transformer 架构，但也有较多的创新来提升性能降低成本。
**1、技术架构革新**
**混合专家模型（MoE）**：将大模型拆分成多个“专家”，训练时分工协作，推理时按需调用，效率提升（类似工厂流水线分工，减少浪费）。
**多头潜在注意力（MLA）**：动态调整注意力焦点，降低内存占用（像用“智能聚光灯”只照亮关键信息）。
**多令牌预测（MTP）**：一次生成多个词，减少重复步骤（比如写文章时直接构思下一段，而非逐字挤牙膏）。
**2、工程优化创新**
**FP8 混合精度训练**：用“简化版数字”做计算，省内存、省算力（类似用速记符号代替长篇大论）。
**底层通信优化**：数据传输效率提升，支持万卡级训练（好比把乡间小路升级成高速公路）。
**3、训练方法创新**
**纯强化学习（RL）驱动**：减少对人工标注数据的依赖，让模型通过试错自我优化（像教孩子骑车，放手让他自己摔几次）。
**低成本训练**：通过技术优化将成本压缩至 550 万美元（约为行业头部企业的 1/3），推动技术普惠。

**问：如何看待 DeepSeek V3 训练成本 550 万美元，是真的吗？**
**答：** DeepSeek 声称其 V3 模型的训练仅仅用了 2048 张显卡，且训练成本仅为 550 万美元，得益于其创新的 MoE 架构和 FP8 混合精度训练等技术，大幅降低了计算资源的需求，同时保持了模型的高性能表现。
虽然训练成本确实标称为 550 万美元，但这仅仅是最终训练阶段的成本，并未包括前期研究、实验验证、数据准备及可能的监督微调等额外开销。因此总体投入更高，但这与其他大模型的训练成本相比仍然是值得关注的进步。

**问：“ 李飞飞团队 50 美元炼出 DeepSeek R1”是真的吗？**
**答：** 类似的消息最近很多，李飞飞团队的模型名为 s1，在数学和编码能力测试中的表现，据传与 OpenAI O1 和 DeepSeek R1 等顶尖推理模型不相上下。李飞飞团队的研究通过精选 1000 条高质量数据并结合“预算强制”技术，在阿里云 Qwen 基座模型上进行监督微调，以约 50 美元的成本提升了模型在特定数学推理任务上的表现，但并未全面超越 DeepSeek R1，其成果依赖于现有基座模型和特定测试集的优化，实际上的技术突破更多体现在小数据高效微调和测试时计算扩展方法上。这样的新闻显然是标题党，但也确实为低成本 AI 训练提供了新思路，同时凸显了开源模型生态的重要性（阿里的 Qwen 是开源模型）。

**问：DeepSeek 使用 2048 张显卡就完成训练是否说明未来市场对显卡需求减少？**
**答：** 未来市场对算力需求不降反增，首先是 Jevons 悖论再现：模型训练提升会刺激其应用的普及，算力总需求暴增（类似手机便宜后用户激增，基站压力更大）。其次 DeepSeek 的成功会进一步导致推理市场爆发：AI 应用落地后，大量实时推理的需求（如智能客服、自动驾驶）将成算力消耗主力。这也可以解释英伟达股价先跌后涨的逻辑。
真正值得关注的是最近 DeepSeek 与国产华为昇腾 910C 和海光 DCU 的适配，这预示着国产算力在推理市场的爆发。

**问：我们常看到的 GPU， TPU，NPU，DCU 都是指什么，有什么区别？**
**答：** 它们都可以用来做深度学习等机器学习任务，但是设计架构和性能略有不同。
**GPU**：原本是用来处理图像和视频的芯片，现在用来加速需要大量并行计算的任务，比如机器学习。
**TPU**：谷歌推出的专门加速机器学习任务的芯片，对大量矩阵运算的任务，效率高于比 GPU。
**NPU**：专门为运行神经网络设计的处理器，能够更高效地执行深度学习算法。
**DCU**：海光推出的机器学习芯片，主要用于加速 AI 和深度学习相关的任务，类似于 GPU 但针对这些领域做了特别优化。

**问：DeepSeek 的模型开源，我们可以自己部署吗？网上很多号称可以本地部署的 7B 或者 8B 的 DeepSeek 模型是”纯血“DeepSeek 吗？**
**答：** 当然可以。
但是完整版的 DeepSeekV3 模型有 671B（即 6710 亿）个参数，模型文件大学有 720GB。而一张 A100 GPU 的显存才 80GB。尽管 DeepSeek 也提供了一些量化后的模型，但是模型文件大小依旧在 100GB 以上。
但同时 DeepSeek 也提供了不同尺寸的蒸馏版模型可以本地部署，所谓蒸馏版模型就是 DeepSeek 使用 DeepSeek-R1 生成的 800,000 个推理数据样本来对较小的基础模型（例如 Qwen 和 Llama 系列）进行微调而创建，为了缩小尺寸对模型进行了进一步量化，如 DeepSeek-R1-Distill-Qwen 系列（1.5B、7B、14B、32B）和 DeepSeek-R1-Distill-Llama 系列（8B、70B）。尽管比原始的 DeepSeek-R1 要小得多，但这些蒸馏模型在各种基准测试中都表现出色，有时甚至能够超越更大的开源模型。这些模型都可以本地部署，所以网上号称的本地部署的 7B 或 8B 模型其实指的是这些蒸馏模型中的一个。

未来技术所人工智能创新中心已经在北研中心内部部署了 DeepSeek R1 模型，可以通过 BATRI 局域网访问，提问内容没有信息外泄风险。人工智能创新中心还将与 103 数字仿真实验室合作为各位同事提供统一的访问接口网址。

（本文由未来技术所人工智能创新中心编撰）
